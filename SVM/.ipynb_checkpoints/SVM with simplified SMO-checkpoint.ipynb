{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as r\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import sklearn.decomposition\n",
    "from skimage.transform import resize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_kernel(X):\n",
    "    \"\"\"\n",
    "    Calculates the kernel using the formula\n",
    "    X * X.T\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : N x d matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    N x N matrix\n",
    "    \"\"\"\n",
    "    return X * X.T\n",
    "\n",
    "def predict(Ki, Y, A, b):\n",
    "    \"\"\"\n",
    "    Calculates the prediction for all inputs\n",
    "    using the formula: \n",
    "    f(x) = sum(A[i] * Y[i] * K(X[i], X)){i=1..N} + b\n",
    "    where K is the kernel function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Ki: 1 x N matrix\n",
    "        Contains the dot product between X[i] and X\n",
    "    Y : N x 1 matrix\n",
    "        Contains the true classes for input data\n",
    "    A : N x 1 matrix \n",
    "        Contains the lagrange multipliers\n",
    "    b : Double\n",
    "        The intercept term\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Double\n",
    "        Calculated hypothesis\n",
    "    \"\"\"\n",
    "    # Calculate Y[i] * K(X[i], X) for all i in\n",
    "    # 1 to N. Returns N x 1 matrix.\n",
    "    inner = np.multiply(Y, Ki.T)\n",
    "    \n",
    "    # Calculate A[i] * { Y[i] * K(X[i], X) } for\n",
    "    # all i in 1 to N. Returns N x 1 matrix.\n",
    "    outer = np.multiply(A, inner)\n",
    "    \n",
    "    return np.sum(outer) + b\n",
    "\n",
    "def get_bounds(Ai, Aj, Yi, Yj, C):\n",
    "    \"\"\"\n",
    "    Specifies the lower and upper bounds based\n",
    "    on Yi and Yj\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Ai: Double\n",
    "        Alpha value for first sample\n",
    "    Aj: Double\n",
    "        Alpha value for second sample\n",
    "    Yi: Integer\n",
    "        True class of first sample\n",
    "    Yj: Integer\n",
    "        True class of second sample\n",
    "    C : Regularization parameter\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Double, Double\n",
    "        The lower and upper bounds\n",
    "    \"\"\"\n",
    "    L = 0  # Lower bound\n",
    "    H = 0  # Upper bound\n",
    "    \n",
    "    # Initialize the bounds\n",
    "    if (Yi == Yj):\n",
    "        L = max(0, Ai + Aj - C)\n",
    "        H = min(C, Ai + Aj)\n",
    "    else:\n",
    "        L = max(0, Aj - Ai)\n",
    "        H = min(C, C + Aj - Ai)\n",
    "    \n",
    "    return L, H\n",
    "\n",
    "def calculate_eta(Xi, Xj):\n",
    "    \"\"\"\n",
    "    Calculates the value of Eta using the\n",
    "    formula: 2 * Xi * Xj.T - Xi * Xi.T -\n",
    "    Xj * Xj.T\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Xi: 1 x d matrix\n",
    "        Contains one input sample\n",
    "    Xj: 1 x d matrix\n",
    "        Contains another input sample\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Double\n",
    "        The calculated value of Eta\n",
    "    \"\"\"\n",
    "    \n",
    "    first = 2 * Xi * Xj.T\n",
    "    second = Xi * Xi.T\n",
    "    third = Xj * Xj.T\n",
    "    \n",
    "    eta = first - second - third\n",
    "    \n",
    "    return eta.getA1()[0]\n",
    "\n",
    "def compute_and_clip_alpha(Aj, Yj, Ei, Ej, eta, L, H):\n",
    "    \"\"\"\n",
    "    Computes the new value for Aj using the\n",
    "    formula: Aj = Aj - Yj * (Ei - Ej) / eta.\n",
    "    Then the value of Aj is clipped to lie\n",
    "    within the range [L, H].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Aj: Double\n",
    "        Alpha value of second sample\n",
    "    Yj: Integer\n",
    "        True class of the second sample\n",
    "    Ei: Double\n",
    "        Prediction error in the first sample\n",
    "    Ej: Double\n",
    "        Prediction error in the second sample\n",
    "    eta:Double\n",
    "        The value of Eta\n",
    "    L : Double\n",
    "        The lower bound\n",
    "    H : Double\n",
    "        The upper bound\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Double\n",
    "        The new value for Aj\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute new Aj\n",
    "    Aj = Aj - (Yj * (Ei - Ej)) / eta\n",
    "    \n",
    "    # Clip Aj to [L, H]\n",
    "    if (Aj > H):\n",
    "        Aj = H\n",
    "    elif (Aj < L):\n",
    "        Aj = L\n",
    "    \n",
    "    return Aj\n",
    "\n",
    "def update_first_alpha(Ai, Aj, Aj_old, Yi, Yj):\n",
    "    \"\"\"\n",
    "    Computes the new alpha value for first\n",
    "    sample using the formula: Ai + Yi * Yj *\n",
    "    (Aj_old - Aj)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Ai: Double\n",
    "        Alpha value for first sample\n",
    "    Aj: Double\n",
    "        Alpha value for second sample\n",
    "    Aj_old: Double\n",
    "        The old alpha value for second sample\n",
    "    Yi: Integer\n",
    "        True class of first sample\n",
    "    Yj: Integer\n",
    "        True class of second sample\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Double\n",
    "        The new alpha value for first sample\n",
    "    \"\"\"    \n",
    "    inner = Yi * Yj * (Aj_old - Aj)\n",
    "    return Ai + inner\n",
    "\n",
    "def compute_threshold(Xi, Xj, Yi, Yj, Ai, Aj, Ai_old, Aj_old, Ei, Ej, b, C):\n",
    "    \"\"\"\n",
    "    Calculates the new value for b threshold\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Xi: 1 x d matrix\n",
    "        Contains one input sample\n",
    "    Xj: 1 x d matrix\n",
    "        Contains another input sample\n",
    "    Yi: Integer\n",
    "        True class of first sample\n",
    "    Yj: Integer\n",
    "        True class of second sample\n",
    "    Ai: Double\n",
    "        Alpha value for first sample\n",
    "    Aj: Double\n",
    "        Alpha value for second sample\n",
    "    Ai_old: Double\n",
    "        The old alpha value for first sample\n",
    "    Aj_old: Double\n",
    "        The old alpha value for second sample\n",
    "    Ei: Double\n",
    "        Prediction error in the first sample\n",
    "    Ej: Double\n",
    "        Prediction error in the second sample\n",
    "    b : Double\n",
    "        Current value of the threshold\n",
    "    C : Double\n",
    "        The regularization parameter\n",
    "    \"\"\"\n",
    "    inputs_dot_product = Xi * Xj.T\n",
    "    inputs_dot_product = inputs_dot_product.getA1()[0]\n",
    "    \n",
    "    # Calculate b1\n",
    "    xi_inner = Xi * Xi.T\n",
    "    xi_inner = xi_inner.getA1()[0]\n",
    "    first = Yi * (Ai - Ai_old) * xi_inner\n",
    "    second = Yj * (Aj - Aj_old) * inputs_dot_product\n",
    "    b1 = b - Ei - first - second\n",
    "    \n",
    "    # Calculate b2\n",
    "    xj_inner = Xj * Xj.T\n",
    "    xj_inner = xj_inner.getA1()[0]\n",
    "    first = Yi * (Ai - Ai_old) * inputs_dot_product\n",
    "    second = Yj * (Aj - Aj_old) * xj_inner\n",
    "    b2 = b - Ej - first - second\n",
    "    \n",
    "    # Calculate the new value for b\n",
    "    if ((0 < Ai) and (Ai < C)):\n",
    "        b = b1\n",
    "    elif ((0 < Aj) and (Aj < C)):\n",
    "        b = b2\n",
    "    else:\n",
    "        b = (b1 + b2) / 2\n",
    "    \n",
    "    return b\n",
    "\n",
    "def simplified_smo(X, Y, C, tol, max_passes):\n",
    "    \"\"\"\n",
    "    Executes the simplified SMO algorithm to\n",
    "    calculate the lagrange multipliers and the\n",
    "    threshold for the solution\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : N x d matrix\n",
    "        The input data\n",
    "    Y : N x 1 matrix\n",
    "        The true values for all inputs\n",
    "    C : Double\n",
    "        The regularization parameter\n",
    "    tol:Double\n",
    "        The numerical tolerance\n",
    "    max_passes : Integer\n",
    "        The maximum number of times to iterate\n",
    "        over alpha's without changing\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A : N x 1 matrix\n",
    "        The lagrange multipliers\n",
    "    b : Double\n",
    "        The threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    N = X.shape[0]  # Number of input samples\n",
    "    d = X.shape[1]  # Number of features\n",
    "    \n",
    "    K = np.dot(X, X.T)  # The kernel\n",
    "    \n",
    "    # Initialize lagrange multipliers and the\n",
    "    # b threshold\n",
    "    A = np.matrix(np.zeros(N)).T  # N x 1\n",
    "    b = 0\n",
    "    \n",
    "    passes = 0\n",
    "    while (passes < max_passes):\n",
    "        \n",
    "        # Initialize variable to keep track of\n",
    "        # the number of alphas that are updated\n",
    "        # in this iteration\n",
    "        num_changed_alphas = 0\n",
    "        \n",
    "        for i in range(N):\n",
    "            \n",
    "            # Extract kernel and true class\n",
    "            # of the i-th input sample\n",
    "            Ki = K[i, :]\n",
    "            Xi = X[i, :]\n",
    "            Yi = Y[i, :].getA1()[0]\n",
    "            \n",
    "            # Calculate prediction error for\n",
    "            # first sample\n",
    "            Pi = predict(Ki, Y, A, b)\n",
    "            Ei = Pi - Yi\n",
    "            \n",
    "            Ai = A[i, :].getA1()[0]\n",
    "            \n",
    "            if ((Yi * Ei < -tol and Ai < C) or\n",
    "                (Yi * Ei > tol and Ai > 0)):\n",
    "                \n",
    "                #print(\"Inside if\")\n",
    "                \n",
    "                # Select j at random such that j != i\n",
    "                while True:\n",
    "                    j = np.random.randint(0, N)\n",
    "                    if (j != i):\n",
    "                        break\n",
    "                \n",
    "                # Extract kernel and true class\n",
    "                # of the i-th input sample\n",
    "                Kj = K[j, :]\n",
    "                Xj = X[j, :]\n",
    "                Yj = Y[j, :].getA1()[0]\n",
    "\n",
    "                # Calculate prediction error for\n",
    "                # first sample\n",
    "                Pj = predict(Kj, Y, A, b)\n",
    "                Ej = Pj - Yj\n",
    "                \n",
    "                Aj = A[j, :].getA1()[0]\n",
    "                \n",
    "                # Save Ai and Aj\n",
    "                Ai_old = Ai\n",
    "                Aj_old = Aj\n",
    "                \n",
    "                # Compute bounds\n",
    "                L, H = get_bounds(Ai, Aj, Yi, Yj, C)\n",
    "                \n",
    "                if (L == H):\n",
    "                    #print(\"L = H. Continue to next i\")\n",
    "                    continue  # Process next i\n",
    "                \n",
    "                # Compute Eta\n",
    "                eta = calculate_eta(Xi, Xj)\n",
    "                \n",
    "                if (eta >= 0):\n",
    "                    #print(\"eta >= 0. Continue to next i\")\n",
    "                    continue  # Process next i\n",
    "                \n",
    "                # Convert A matrix to an array temporarily for updation\n",
    "                A = A.getA1()\n",
    "                \n",
    "                # Compute new Aj\n",
    "                Aj = compute_and_clip_alpha(Aj, Yj, Ei, Ej, eta, L, H)\n",
    "                A[j] = Aj\n",
    "                \n",
    "                if (abs(Aj - Aj_old) < math.pow(10, -5)):\n",
    "                    A = np.matrix(A).T\n",
    "                    continue  # Process next i\n",
    "                \n",
    "                # Calculate new Ai\n",
    "                Ai = update_first_alpha(Ai, Aj, Aj_old, Yi, Yj)\n",
    "                A[i] = Ai\n",
    "                \n",
    "                # Convert A back to a N x 1 matrix\n",
    "                A = np.matrix(A).T\n",
    "                \n",
    "                # Calculate new b threshold\n",
    "                b = compute_threshold(Xi, Xj, Yi, Yj, Ai, Aj, Ai_old, Aj_old, Ei, Ej, b, C)\n",
    "                \n",
    "                num_changed_alphas += 1\n",
    "                \n",
    "            # end if\n",
    "            \n",
    "        # end for\n",
    "        \n",
    "        if (num_changed_alphas == 0):\n",
    "            passes += 1\n",
    "        else:\n",
    "            passes = 0\n",
    "            \n",
    "    # end while\n",
    "    \n",
    "    return A, b\n",
    "\n",
    "def classify(A, Y_trn, X_trn, X, b):\n",
    "    \"\"\"\n",
    "    Predicts the classes for each input data point\n",
    "    in X using the formula: A[i] * Y_trn[i] * \n",
    "    (X_trn[i] * X) + b. The prediction is 1 if \n",
    "    this calculate value is > 0. Else 0.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : N_trn x 1 matrix\n",
    "        The optimized Lagrangian multipliers\n",
    "    Y_trn : N_trn x 1 matrix\n",
    "        The true classes for training data\n",
    "    X_trn : N_trn x d matrix\n",
    "        The features for training data\n",
    "    X : N x d matrix\n",
    "        The features for data which needs to be\n",
    "        classified\n",
    "    b : Double\n",
    "        The threshold\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    N x 1 matrix\n",
    "        Containing the predicted classes for all\n",
    "        input data points\n",
    "    \"\"\"\n",
    "    \n",
    "    K = X_trn * X.T  # The kernel - N_trn x N\n",
    "    \n",
    "    # Calculate Y_trn[i] * <X_trn[i], X> for all i\n",
    "    inner = np.multiply(Y_trn, K)\n",
    "    \n",
    "    # Multiply lagrange multipliers and add the\n",
    "    # threshold\n",
    "    outer = np.multiply(A, inner)\n",
    "    outer = np.matrix(np.sum(outer, axis = 0)).T\n",
    "    outer = outer + b\n",
    "    \n",
    "    # Calculate the final prediction\n",
    "    return vector_indicator(outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path, col_name):\n",
    "    \"\"\"\n",
    "    Load input data from matlab file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : String\n",
    "        The relative path of matlab file    \n",
    "    col_name : String\n",
    "        Label of the input data within the\n",
    "        file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    N x D matrix\n",
    "        Contains the resized image data\n",
    "        where each column represents an\n",
    "        image\n",
    "    \n",
    "    N x num_labels matrix\n",
    "        Contains the one-hot encoded form\n",
    "        of each data point's classification\n",
    "    \"\"\"\n",
    "    resize_width = 17\n",
    "    resize_height = 20\n",
    "    \n",
    "    ip = sio.loadmat(path)\n",
    "\n",
    "    N = ip[col_name].shape[1] * ip[col_name][:, 0][0].shape[2]\n",
    "    num_labels = ip[col_name].shape[1]\n",
    "\n",
    "    size = (resize_height, resize_width)\n",
    "    X = np.zeros((N, resize_height * resize_width))\n",
    "    Y = np.full((N, num_labels), -1)\n",
    "\n",
    "    img_index = 0\n",
    "\n",
    "    for i in range(num_labels):\n",
    "        curr_class_data = ip[col_name][:,i][0]\n",
    "        for j in range(curr_class_data.shape[2]):\n",
    "            img_resized = resize(curr_class_data[:,:,j], size, mode='constant')\n",
    "            X[img_index, :] = img_resized.flatten()\n",
    "            Y[img_index, i] = 1\n",
    "            img_index += 1\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def pre_process(data, y_col):\n",
    "    \"\"\"\n",
    "    Split the input data into a matrix that\n",
    "    contains all features and a matrix that\n",
    "    contains all output classes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Pandas DataFrame\n",
    "        Contains the input data\n",
    "    y_col : String\n",
    "        Header of the result column\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    N x d matrix\n",
    "        Storing d features for N samples\n",
    "    N x 1 matrix\n",
    "        Storing the classification for N\n",
    "        samples\n",
    "    \"\"\"    \n",
    "    data_cols = list(data.columns.values)\n",
    "    data_cols.remove(y_col)\n",
    "    \n",
    "    X = data[data_cols]\n",
    "    X = np.matrix(X.values)\n",
    "    \n",
    "    Y = data[[y_col]]\n",
    "    Y = vector_class_update(np.matrix(Y.values))\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def indicator(score):\n",
    "    \"\"\"\n",
    "    Indicator function which returns 1\n",
    "    if the score is > 0. Else, returns\n",
    "    0.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    score : Double        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Integer (either 0 or 1)\n",
    "    \"\"\"\n",
    "    return 1 if (score > 0) else -1\n",
    "\n",
    "vector_indicator = np.vectorize(indicator)\n",
    "\n",
    "def class_update(true_class):\n",
    "    \"\"\"\n",
    "    Updates the true classes \n",
    "    \"\"\"\n",
    "    return 1 if (true_class != 0) else -1\n",
    "\n",
    "vector_class_update = np.vectorize(class_update)\n",
    "\n",
    "def plot_data(data, y_col, y_pred):\n",
    "    \"\"\"\n",
    "    Renders a scatter plot for the data\n",
    "    passed in and colors the nodes based\n",
    "    on their predicted classes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Pandas DataFrame\n",
    "        Contains the input data along with\n",
    "        their predicted classes\n",
    "    y_col : String\n",
    "        The name of the column storing true\n",
    "        classes\n",
    "    y_pred : String\n",
    "        The name of the column storing the\n",
    "        predicted classes\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    \n",
    "    # Draw the plots\n",
    "    ax.scatter(data.x1, data.x2, label='Data', c=data[[y_pred]])\n",
    "    \n",
    "    # Set extra properties for readability\n",
    "    ax.legend(loc=2)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_title('x1 vs. x2')\n",
    "    \n",
    "    # Set the x and y axis limits for the plot\n",
    "    x1_min = data.x1.min()\n",
    "    x1_max = data.x1.max()\n",
    "    ax.set_xlim(x1_min + 0.2 * x1_min, x1_max + + 0.2 * x1_max)\n",
    "    \n",
    "    x2_min = data.x2.min()\n",
    "    x2_max = data.x2.max()\n",
    "    ax.set_ylim(x2_min + 0.2 * x2_min, x2_max + + 0.2 * x2_max)\n",
    "\n",
    "def plot_misclassification_error(C, errors):\n",
    "    plt.plot(C, errors, 'k-')\n",
    "    plt.xlabel('C')\n",
    "    plt.ylabel('Misclassifications')\n",
    "    plt.xscale('log')\n",
    "    #plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def execute3b():\n",
    "    \n",
    "    path = \"ExtYaleB10.mat\"\n",
    "    col_name = 'train'\n",
    "\n",
    "    C = 10\n",
    "    tol = -1\n",
    "    max_passes = 10\n",
    "\n",
    "    X_trn, Y_trn = load_data(path, col_name)\n",
    "    X_trn = np.matrix(X_trn)\n",
    "    \n",
    "    # ----------------------- Train the 10 classifiers -------------------------\n",
    "    \n",
    "    classifiers = []\n",
    "    for i in range(10):\n",
    "        print(\"Training classifier #\", i)\n",
    "        curr_Y_trn = np.matrix(np.reshape(Y_trn[:, i], (Y_trn.shape[0], 1)))\n",
    "        A, b = simplified_smo(X_trn, curr_Y_trn, C, tol, max_passes)\n",
    "        classifiers.append((A, b))\n",
    "    \n",
    "    # ---------------------- Run the model on test data ------------------------\n",
    "\n",
    "    col_name = \"test\"\n",
    "\n",
    "    X_tst, Y_tst = load_data(path, col_name)\n",
    "    X_tst = np.matrix(X_tst)\n",
    "\n",
    "    misclassifications = 0\n",
    "\n",
    "    for i in range(10):    \n",
    "        curr_Y_trn = np.matrix(np.reshape(Y_trn[:, i], (Y_trn.shape[0], 1)))\n",
    "        curr_Y_tst = np.matrix(np.reshape(Y_tst[:, i], (Y_tst.shape[0], 1)))\n",
    "\n",
    "        A, b = classifiers[i]\n",
    "\n",
    "        Y_pred = classify(A, curr_Y_trn, X_trn, X_tst, b)\n",
    "\n",
    "        Y_tst_arr = curr_Y_tst.getA1()\n",
    "        Y_pred_arr = Y_pred.getA1()\n",
    "\n",
    "        misclassifications += np.sum(Y_tst_arr != Y_pred_arr)\n",
    "\n",
    "    print(\"Total Misclassifications in test data: \", misclassifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
