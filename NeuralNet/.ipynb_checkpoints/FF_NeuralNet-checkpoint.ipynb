{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FNN(object):\n",
    "    def __init__(self, activation=None, output_dim=10, learning_rate = 0.01, lamda = 0.01):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating D x N matrix from the given data\n",
    "# Downsample it to 20 x 17 to reduce computation cost\n",
    "def load_data(path, col_name):\n",
    "    resize_width = 17\n",
    "    resize_height = 20\n",
    "    \n",
    "    data = sio.loadmat(path)\n",
    "\n",
    "    N = data[col_name].shape[1] * data[col_name][:, 0][0].shape[2]\n",
    "    num_labels = data[col_name].shape[1]\n",
    "\n",
    "    size = (resize_height, resize_width)\n",
    "    X = np.zeros((N, resize_height * resize_width))\n",
    "    Y = np.zeros((N, num_labels))\n",
    "\n",
    "    img_index = 0\n",
    "\n",
    "    for i in range(num_labels):\n",
    "        curr_class_data = data[col_name][:,i][0]\n",
    "        for j in range(curr_class_data.shape[2]):\n",
    "            img_resized = resize(curr_class_data[:,:,j], size, mode='constant')\n",
    "            X[img_index, :] = img_resized.flatten()\n",
    "            Y[img_index, i] = 1\n",
    "            img_index += 1\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the accuracy of the predictions vs given labels\n",
    "def accuracy(predictions, labels):\n",
    "    preds_correct_boolean =  np.argmax(predictions, 1) == np.argmax(labels, 1)\n",
    "    correct_predictions = np.sum(preds_correct_boolean)\n",
    "    accuracy = 100.0 * correct_predictions / predictions.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''Sigmoid function of x.'''\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_softmax_loss_array(softmax_probs_array, y_onehot):\n",
    "    indices = np.argmax(y_onehot, axis = 1).astype(int)\n",
    "    predicted_probability = softmax_probs_array[np.arange(len(softmax_probs_array)), indices]\n",
    "    log_preds = np.log(predicted_probability)\n",
    "    loss = -1.0 * np.sum(log_preds) / len(log_preds)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regularization_L2(reg_lambda, weight1, weight2):\n",
    "    weight1_loss = 0.5 * reg_lambda * np.sum(weight1 * weight1)\n",
    "    weight2_loss = 0.5 * reg_lambda * np.sum(weight2 * weight2)\n",
    "    return weight1_loss + weight2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(data, label, layer1_weights_array, layer2_weights_array, layer1_biases_array, layer2_biases_array):\n",
    "    input_layer = np.dot(data, layer1_weights_array)\n",
    "    hidden_layer = np.tanh(input_layer + layer1_biases_array)\n",
    "    scores = np.dot(hidden_layer, layer2_weights_array) + layer2_biases_array\n",
    "    final_activations = softmax(scores)\n",
    "    print('Test accuracy: {0}%'.format(accuracy(final_activations, label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(output_array):\n",
    "    logits_exp = np.exp(output_array)\n",
    "    return logits_exp / np.sum(logits_exp, axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_activation(data_array):\n",
    "    return np.maximum(data_array, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh_derivative(data):\n",
    "    th = np.tanh(data)\n",
    "    return 1 - th*th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FFN(data, label, hidden_nodes):\n",
    "    training_data = data\n",
    "    training_labels = label\n",
    "    \n",
    "    num_labels = training_labels.shape[1]\n",
    "    num_features = training_data.shape[1]\n",
    "    learning_rate = .01\n",
    "    reg_lambda = .01\n",
    "\n",
    "    # Weights and Bias Arrays, just like in Tensorflow\n",
    "    layer1_weights_array = np.random.normal(0, 1, [num_features, hidden_nodes]) \n",
    "    layer2_weights_array = np.random.normal(0, 1, [hidden_nodes, num_labels]) \n",
    "\n",
    "    layer1_biases_array = np.zeros((1, hidden_nodes))\n",
    "    layer2_biases_array = np.zeros((1, num_labels))\n",
    "\n",
    "\n",
    "    for step in range(20000):\n",
    "\n",
    "        input_layer = np.dot(training_data, layer1_weights_array)\n",
    "        hidden_layer = np.tanh(input_layer + layer1_biases_array)\n",
    "        output_layer = np.dot(hidden_layer, layer2_weights_array) + layer2_biases_array\n",
    "        output_probs = softmax(output_layer)\n",
    "\n",
    "        loss = cross_entropy_softmax_loss_array(output_probs, training_labels)\n",
    "        loss += regularization_L2(reg_lambda, layer1_weights_array, layer2_weights_array)\n",
    "\n",
    "        output_error_signal = (output_probs - training_labels) / output_probs.shape[0]\n",
    "\n",
    "        error_signal_hidden = np.dot(output_error_signal, layer2_weights_array.T)\n",
    "        error_signal_hidden = tanh_derivative(error_signal_hidden)\n",
    "#         error_signal_hidden[hidden_layer <= 0] = 0\n",
    "\n",
    "        gradient_layer2_weights = np.dot(hidden_layer.T, output_error_signal)\n",
    "        gradient_layer2_bias = np.sum(output_error_signal, axis = 0, keepdims = True)\n",
    "\n",
    "        gradient_layer1_weights = np.dot(training_data.T, error_signal_hidden)\n",
    "        gradient_layer1_bias = np.sum(error_signal_hidden, axis = 0, keepdims = True)\n",
    "\n",
    "        gradient_layer2_weights += reg_lambda * layer2_weights_array\n",
    "        gradient_layer1_weights += reg_lambda * layer1_weights_array\n",
    "\n",
    "        layer1_weights_array -= learning_rate * gradient_layer1_weights\n",
    "        layer1_biases_array -= learning_rate * gradient_layer1_bias\n",
    "        layer2_weights_array -= learning_rate * gradient_layer2_weights\n",
    "        layer2_biases_array -= learning_rate * gradient_layer2_bias\n",
    "\n",
    "        if step % 500 == 0:\n",
    "                print('Loss at step {0}: {1}'.format(step, loss))\n",
    "            \n",
    "    return layer1_weights_array, layer2_weights_array, layer1_biases_array, layer2_biases_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Train Data\n",
    "    train_data, train_label = load_data(\"ExtYaleB10.mat\", 'train')\n",
    "    layer1_weights_array, layer2_weights_array, layer1_biases_array, layer2_biases_array = FFN(train_data, train_label, 5)\n",
    "    calculate_accuracy(train_data, train_label, layer1_weights_array, layer2_weights_array, layer1_biases_array, layer2_biases_array)\n",
    "    # Test Data\n",
    "    test_data, test_label = load_data(\"ExtYaleB10.mat\", \"test\")\n",
    "    calculate_accuracy(test_data, test_label, layer1_weights_array, layer2_weights_array, layer1_biases_array, layer2_biases_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 11.807061623907483\n",
      "Loss at step 500: 4227897.623297385\n",
      "Loss at step 1000: 16096840.986152098\n",
      "Loss at step 1500: 34487370.68624332\n",
      "Loss at step 2000: 58405685.46594644\n",
      "Loss at step 2500: 86970760.4614202\n",
      "Loss at step 3000: 119402727.08675358\n",
      "Loss at step 3500: 155012402.05658588\n",
      "Loss at step 4000: 193191854.0614561\n",
      "Loss at step 4500: 233405907.33945417\n",
      "Loss at step 5000: 275184491.074688\n",
      "Loss at step 5500: 318115752.3127264\n",
      "Loss at step 6000: 361839858.0046627\n",
      "Loss at step 6500: 406043418.9547768\n",
      "Loss at step 7000: 450454474.9243346\n",
      "Loss at step 7500: 494837986.0012883\n",
      "Loss at step 8000: 538991780.64177\n",
      "Loss at step 8500: 582742915.5778604\n",
      "Loss at step 9000: 625944407.1156576\n",
      "Loss at step 9500: 668472297.2619847\n",
      "Loss at step 10000: 710223021.6568615\n",
      "Loss at step 10500: 751111049.487981\n",
      "Loss at step 11000: 791066768.4553611\n",
      "Loss at step 11500: 830034590.4684374\n",
      "Loss at step 12000: 867971256.1207006\n",
      "Loss at step 12500: 904844318.1225922\n",
      "Loss at step 13000: 940630785.803489\n",
      "Loss at step 13500: 975315914.5379038\n",
      "Loss at step 14000: 1008892125.5272853\n",
      "Loss at step 14500: 1041358042.7931106\n",
      "Loss at step 15000: 1072717635.5238892\n",
      "Loss at step 15500: 1102979455.081388\n",
      "Loss at step 16000: 1132155957.021757\n",
      "Loss at step 16500: 1160262899.436069\n",
      "Loss at step 17000: 1187318809.7717953\n",
      "Loss at step 17500: 1213344513.070766\n",
      "Loss at step 18000: 1238362715.2581534\n",
      "Loss at step 18500: 1262397635.748185\n",
      "Loss at step 19000: 1285474684.2021592\n",
      "Loss at step 19500: 1307620176.7887964\n",
      "Test accuracy: 10.0%\n",
      "Test accuracy: 10.0%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
